import numpy as np
from tqdm import tqdm
from game.big2Game import big2Game
from agents import RandomAgent, CNNAgent, Agent, Big2CNN
from typing import List
import torch
import multiprocessing
from itertools import repeat
import cProfile
import pstats
import io
import pandas as pd
import time


def profile(fnc):
    """A decorator that uses cProfile to profile a function"""

    def inner(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        retval = fnc(*args, **kwargs)
        pr.disable()
        s = io.StringIO()
        sortby = "cumulative"
        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
        ps.print_stats()
        print(s.getvalue())
        return retval

    return inner


def create_training_csv(csv_file_path):
    """å‰µå»ºè¨“ç·´è¨˜éŒ„CSVæ–‡ä»¶"""
    columns = [
        'episode',
        'timestamp',
        'training_cnn_win_rate',
        'training_cnn_avg_reward',
        'training_best_win_rate',
        'training_best_avg_reward',
        'eval_cnn_win_rate',
        'eval_cnn_avg_reward',
        'eval_random1_win_rate',
        'eval_random1_avg_reward',
        'eval_random2_win_rate',
        'eval_random2_avg_reward',
        'eval_random3_win_rate',
        'eval_random3_avg_reward',
        'model_improved',
        'best_win_rate_so_far'
    ]
    
    df = pd.DataFrame(columns=columns)
    df.to_csv(csv_file_path, index=False)
    return csv_file_path


def log_training_episode(csv_file_path, episode_data):
    """è¨˜éŒ„å–®å€‹episodeçš„è¨“ç·´æ•¸æ“šåˆ°CSV"""
    df = pd.DataFrame([episode_data])
    
    # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œè¿½åŠ æ•¸æ“šï¼›å¦å‰‡å‰µå»ºæ–°æ–‡ä»¶
    try:
        existing_df = pd.read_csv(csv_file_path)
        updated_df = pd.concat([existing_df, df], ignore_index=True)
    except FileNotFoundError:
        updated_df = df
    
    updated_df.to_csv(csv_file_path, index=False)


def run_game(agent: Agent) -> List[float]:
    """é€²è¡Œä¸€å ´éŠæˆ²ï¼Œä¸€å€‹CNNä»£ç†å°æŠ—ä¸‰å€‹éš¨æ©Ÿä»£ç†"""
    game = big2Game()
    agent.reset()

    agents: List[Agent] = [
        agent,
        RandomAgent(),
        RandomAgent(),
        RandomAgent(),
    ]

    while not game.isGameOver():
        player_go, first_player, history, hand, avail_actions = game.getCurrentState()
        action = agents[player_go].step(first_player, history, hand, avail_actions)
        game.step(action)

    rewards = game.getRewards()

    # éŠæˆ²çµæŸï¼Œè¿”å›çå‹µ
    return rewards

def train_single_game(agent: Agent, best_agent: Agent):
    """è¨“ç·´å–®å ´éŠæˆ²"""
    game = big2Game()
    agent.reset()
    best_agent.reset()
    agents: List[Agent] = [
        agent,
        best_agent,
        best_agent,
        best_agent,
    ]

    while not game.isGameOver():
        player_go, first_player, history, hand, avail_actions = game.getCurrentState()
        action = agents[player_go].step(first_player, history, hand, avail_actions)
        game.step(action)

    rewards = game.getRewards()
    agent.set_final_reward(rewards[0])

    return rewards


def evaluate_model(agent: Agent, num_games=100):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    total_rewards = np.zeros(4, dtype=float)
    wins = np.zeros(4, dtype=int)

    cpu_count = 4

    with multiprocessing.Pool(processes=cpu_count) as pool:
        results = list(
            tqdm(
                pool.imap(run_game, repeat(agent, num_games)),
                total=num_games,
                desc="è©•ä¼°ä¸­",
            )
        )

    for rewards in results:
        total_rewards += rewards
        winner = np.argmax(rewards)
        wins[winner] += 1

    avg_rewards = total_rewards / num_games
    win_rates = wins / num_games * 100
    return avg_rewards, win_rates, wins


if __name__ == "__main__":
    num_games = 100
    training_games = 100  # Number of games to train before evaluation

    # å‰µå»ºè¨“ç·´è¨˜éŒ„æ–‡ä»¶
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    csv_file_path = f"training_log_{timestamp}.csv"
    create_training_csv(csv_file_path)
    print(f"ğŸ“ è¨“ç·´è¨˜éŒ„å°‡ä¿å­˜åˆ°: {csv_file_path}")

    training_model = Big2CNN()
    # training_model.load_state_dict(torch.load("CNN_agent_best.pt")) # Comment this line if you want to start training from scratch
    training_model.train()

    best_model = Big2CNN()
    # best_model.load_state_dict(torch.load("CNN_agent_best.pt"))
    best_model.eval()

    multiprocessing.set_start_method("spawn")

    print("é–‹å§‹è¨“ç·´å’Œè©•ä¼°å¾ªç’°...")

    best_win_rate = 0.0
    episode = 0
    
    try:
        while True:
            episode += 1
            episode_start_time = time.time()
            print(f"\n========== Episode {episode} ==========")

            # Training phase
            print(f"è¨“ç·´éšæ®µï¼šé€²è¡Œ {training_games} å ´éŠæˆ²...")
            training_rewards = []
            training_agent = CNNAgent(training_model, train=True)
            best_agent = CNNAgent(best_model, train=False)
            for i in tqdm(range(training_games), desc="è¨“ç·´ä¸­"):
                rewards = train_single_game(training_agent, best_agent)
                training_rewards.append(rewards)

            # Calculate training statistics
            training_rewards = np.array(training_rewards)
            avg_training_rewards = np.mean(training_rewards, axis=0)
            training_wins = np.sum(
                training_rewards == np.max(training_rewards, axis=1, keepdims=True), axis=0
            )
            training_win_rates = training_wins / training_games * 100

            print(
                f"è¨“ç·´çµæœ - CNNç²å‹ç‡: {training_win_rates[0]:.1f}%, å¹³å‡çå‹µ: {avg_training_rewards[0]:.2f}"
            )

            # Evaluation phase
            print(f"è©•ä¼°éšæ®µï¼šé€²è¡Œ {num_games} å ´éŠæˆ²...")
            eval_agent = CNNAgent(training_model, train=False)
            avg_rewards, win_rates, wins = evaluate_model(eval_agent, num_games)

            # è¼¸å‡ºçµæœ
            print("\n----------- è©•ä¼°çµæœçµ±è¨ˆ -----------")
            print(f"ç¸½å ´æ•¸: {num_games}å ´")
            print("\nç©å®¶ç²å‹æ¬¡æ•¸:")
            print(f"ç©å®¶ 1 (CNN): {int(wins[0])}å ´ ({win_rates[0]:.2f}%)")
            for i in range(1, 4):
                print(f"ç©å®¶ {i + 1} (éš¨æ©Ÿ): {int(wins[i])}å ´ ({win_rates[i]:.2f}%)")

            print("\nå¹³å‡çå‹µ:")
            print(f"ç©å®¶ 1 (CNN): {avg_rewards[0]:.2f}")
            for i in range(1, 4):
                print(f"ç©å®¶ {i + 1} (éš¨æ©Ÿ): {avg_rewards[i]:.2f}")

            # æª¢æŸ¥æ˜¯å¦æœ‰æ”¹é€²
            model_improved = False
            if win_rates[0] > best_win_rate:  # If CNN wins more than previous best
                best_win_rate = win_rates[0]
                best_model.load_state_dict(training_model.state_dict())
                torch.save(training_model.state_dict(), "CNN_agent_best.pt")
                model_improved = True
                print("ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼")

            # è¨˜éŒ„åˆ°CSV
            episode_data = {
                'episode': episode,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'training_cnn_win_rate': training_win_rates[0],
                'training_cnn_avg_reward': avg_training_rewards[0],
                'training_best_win_rate': training_win_rates[1],  # best agentåœ¨ä½ç½®1
                'training_best_avg_reward': avg_training_rewards[1],
                'eval_cnn_win_rate': win_rates[0],
                'eval_cnn_avg_reward': avg_rewards[0],
                'eval_random1_win_rate': win_rates[1],
                'eval_random1_avg_reward': avg_rewards[1],
                'eval_random2_win_rate': win_rates[2],
                'eval_random2_avg_reward': avg_rewards[2],
                'eval_random3_win_rate': win_rates[3],
                'eval_random3_avg_reward': avg_rewards[3],
                'model_improved': model_improved,
                'best_win_rate_so_far': best_win_rate
            }
            
            log_training_episode(csv_file_path, episode_data)
            
            episode_time = time.time() - episode_start_time
            print(f"â±ï¸  Episode {episode} å®Œæˆï¼Œè€—æ™‚: {episode_time:.1f}ç§’")
            print(f"ğŸ“Š æ•¸æ“šå·²è¨˜éŒ„åˆ°: {csv_file_path}")

    except KeyboardInterrupt:
        print("\nâš ï¸  è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        print(f"ğŸ“ è¨“ç·´è¨˜éŒ„å·²ä¿å­˜åˆ°: {csv_file_path}")
    except Exception as e:
        print(f"\nâŒ è¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        print(f"ğŸ“ è¨“ç·´è¨˜éŒ„å·²ä¿å­˜åˆ°: {csv_file_path}")
        import traceback
        traceback.print_exc()
