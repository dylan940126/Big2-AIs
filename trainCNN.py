import numpy as np
from tqdm import tqdm
from game.big2Game import big2Game
from agents import RandomAgent, CNNAgent, Agent, Big2CNN
from typing import List
import torch
import multiprocessing
from itertools import repeat
import cProfile
import pstats
import io


def profile(fnc):
    """A decorator that uses cProfile to profile a function"""

    def inner(*args, **kwargs):
        pr = cProfile.Profile()
        pr.enable()
        retval = fnc(*args, **kwargs)
        pr.disable()
        s = io.StringIO()
        sortby = "cumulative"
        ps = pstats.Stats(pr, stream=s).sort_stats(sortby)
        ps.print_stats()
        print(s.getvalue())
        return retval

    return inner


def run_game(agent: Agent) -> List[float]:
    """é€²è¡Œä¸€å ´éŠæˆ²ï¼Œä¸€å€‹CNNä»£ç†å°æŠ—ä¸‰å€‹éš¨æ©Ÿä»£ç†"""
    game = big2Game()
    agent.reset()

    agents: List[Agent] = [
        agent,
        RandomAgent(),
        RandomAgent(),
        RandomAgent(),
    ]

    while not game.isGameOver():
        player_go, first_player, history, hand, avail_actions = game.getCurrentState()
        action = agents[player_go].step(first_player, history, hand, avail_actions)
        game.step(action)

    rewards = game.getRewards()

    # éŠæˆ²çµæŸï¼Œè¿”å›çå‹µ
    return rewards

def train_single_game(agent: Agent, best_agent: Agent):
    """è¨“ç·´å–®å ´éŠæˆ²"""
    game = big2Game()
    agent.reset()
    best_agent.reset()
    agents: List[Agent] = [
        agent,
        best_agent,
        best_agent,
        best_agent,
    ]

    while not game.isGameOver():
        player_go, first_player, history, hand, avail_actions = game.getCurrentState()
        action = agents[player_go].step(first_player, history, hand, avail_actions)
        game.step(action)

    rewards = game.getRewards()
    agent.set_final_reward(rewards[0])

    return rewards


def evaluate_model(agent: Agent, num_games=100):
    """è©•ä¼°æ¨¡å‹æ€§èƒ½"""
    total_rewards = np.zeros(4, dtype=float)
    wins = np.zeros(4, dtype=int)

    cpu_count = 4

    with multiprocessing.Pool(processes=cpu_count) as pool:
        results = list(
            tqdm(
                pool.imap(run_game, repeat(agent, num_games)),
                total=num_games,
                desc="è©•ä¼°ä¸­",
            )
        )

    for rewards in results:
        total_rewards += rewards
        winner = np.argmax(rewards)
        wins[winner] += 1

    avg_rewards = total_rewards / num_games
    win_rates = wins / num_games * 100
    return avg_rewards, win_rates, wins


if __name__ == "__main__":
    num_games = 100
    training_games = 100  # Number of games to train before evaluation

    training_model = Big2CNN()
    # training_model.load_state_dict(torch.load("cnn_agent_best.pt")) # Comment this line if you want to start training from scratch
    training_model.train()

    best_model = Big2CNN()
    # best_model.load_state_dict(torch.load("cnn_agent_best.pt"))
    best_model.eval()

    multiprocessing.set_start_method("spawn")

    print("é–‹å§‹è¨“ç·´å’Œè©•ä¼°å¾ªç’°...")

    best_win_rate = 0.0
    episode = 0
    while True:
        episode += 1
        print(f"\n========== Episode {episode} ==========")

        # Training phase
        print(f"è¨“ç·´éšæ®µï¼šé€²è¡Œ {training_games} å ´éŠæˆ²...")
        training_rewards = []
        training_agent = CNNAgent(training_model, train=True)
        best_agent = CNNAgent(best_model, train=False)
        for i in tqdm(range(training_games), desc="è¨“ç·´ä¸­"):
            rewards = train_single_game(training_agent, best_agent)
            training_rewards.append(rewards)

        # Calculate training statistics
        training_rewards = np.array(training_rewards)
        avg_training_rewards = np.mean(training_rewards, axis=0)
        training_wins = np.sum(
            training_rewards == np.max(training_rewards, axis=1, keepdims=True), axis=0
        )
        training_win_rates = training_wins / training_games * 100

        print(
            f"è¨“ç·´çµæœ - CNNç²å‹ç‡: {training_win_rates[0]:.1f}%, å¹³å‡çå‹µ: {avg_training_rewards[0]:.2f}"
        )

        # Evaluation phase
        print(f"è©•ä¼°éšæ®µï¼šé€²è¡Œ {num_games} å ´éŠæˆ²...")
        eval_agent = CNNAgent(training_model, train=False)
        avg_rewards, win_rates, wins = evaluate_model(eval_agent, num_games)

        # è¼¸å‡ºçµæœ
        print("\n----------- è©•ä¼°çµæœçµ±è¨ˆ -----------")
        print(f"ç¸½å ´æ•¸: {num_games}å ´")
        print("\nç©å®¶ç²å‹æ¬¡æ•¸:")
        print(f"ç©å®¶ 1 (CNN): {int(wins[0])}å ´ ({win_rates[0]:.2f}%)")
        for i in range(1, 4):
            print(f"ç©å®¶ {i + 1} (éš¨æ©Ÿ): {int(wins[i])}å ´ ({win_rates[i]:.2f}%)")

        print("\nå¹³å‡çå‹µ:")
        print(f"ç©å®¶ 1 (CNN): {avg_rewards[0]:.2f}")
        for i in range(1, 4):
            print(f"ç©å®¶ {i + 1} (éš¨æ©Ÿ): {avg_rewards[i]:.2f}")

        # Save best model if CNN win rate is improving
        if win_rates[0] > best_win_rate:  # If CNN wins more than previous best
            best_win_rate = win_rates[0]
            best_model.load_state_dict(training_model.state_dict())
            torch.save(training_model.state_dict(), "cnn_agent_best.pt")
            print("ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼")
