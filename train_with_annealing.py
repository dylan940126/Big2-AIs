#!/usr/bin/env python3
"""
è¨“ç·´è…³æœ¬ï¼šä½¿ç”¨ epsilon å’Œå­¸ç¿’ç‡é€€ç«çš„ CNN Agent è¨“ç·´
"""

import torch
import numpy as np
from tqdm import tqdm
from game.big2Game import big2Game
from agents import RandomAgent, CNNAgent, Big2CNN
import matplotlib.pyplot as plt
import json
from datetime import datetime


def train_with_annealing():
    """ä½¿ç”¨è¶…åƒæ•¸é€€ç«é€²è¡Œè¨“ç·´"""
    
    # è¨“ç·´åƒæ•¸
    NUM_EPISODES = 1000
    EVAL_INTERVAL = 50  # æ¯50å€‹episodeè©•ä¼°ä¸€æ¬¡
    NUM_EVAL_GAMES = 100  # è©•ä¼°æ™‚é€²è¡Œçš„éŠæˆ²æ•¸é‡
    SAVE_INTERVAL = 100  # æ¯100å€‹episodeä¿å­˜ä¸€æ¬¡æ¨¡å‹
    
    # CNN Agent åƒæ•¸ (ä½¿ç”¨æ–°çš„é€€ç«åŠŸèƒ½)
    cnn_params = {
        'train': True,
        'epsilon': 0.3,          # åˆå§‹æ¢ç´¢ç‡è¼ƒé«˜
        'epsilon_min': 0.01,     # æœ€å°æ¢ç´¢ç‡
        'epsilon_decay': 0.995,  # æ¢ç´¢ç‡è¡°æ¸›
        'lr': 1e-3,              # åˆå§‹å­¸ç¿’ç‡
        'lr_min': 1e-6,          # æœ€å°å­¸ç¿’ç‡
        'lr_decay': 0.999,       # å­¸ç¿’ç‡è¡°æ¸›
        'gamma': 0.95            # æŠ˜æ‰£å› å­
    }
    
    # å‰µå»º CNN Agent
    cnn_agent = CNNAgent(model=None, **cnn_params)
    
    # è¨“ç·´è¨˜éŒ„
    training_log = {
        'episodes': [],
        'win_rates': [],
        'avg_rewards': [],
        'epsilon_values': [],
        'lr_values': [],
        'losses': []
    }
    
    print(f"é–‹å§‹è¨“ç·´ï¼Œå…± {NUM_EPISODES} å€‹ episodes")
    print(f"åˆå§‹åƒæ•¸: epsilon={cnn_params['epsilon']}, lr={cnn_params['lr']}")
    
    best_win_rate = 0.0
    
    for episode in tqdm(range(NUM_EPISODES), desc="è¨“ç·´é€²åº¦"):
        # è¨“ç·´ä¸€å±€éŠæˆ²
        game = big2Game()
        agents = [cnn_agent, RandomAgent(), RandomAgent(), RandomAgent()]
        
        # é‡ç½® agents
        for agent in agents:
            agent.reset()
        
        # é€²è¡ŒéŠæˆ²
        while not game.isGameOver():
            player_go, first_player, history, hand, avail_actions = game.getCurrentState()
            action = agents[player_go].step(first_player, history, hand, avail_actions)
            game.step(action)
        
        # åˆ†é…çå‹µ
        game.assignRewards()
        rewards = game.rewards
        
        # è¨­ç½®æœ€çµ‚çå‹µï¼ˆåªæœ‰è¨“ç·´ä¸­çš„ agent éœ€è¦ï¼‰
        if hasattr(agents[0], 'set_final_reward'):
            agents[0].set_final_reward(rewards[0])
        
        # è¨˜éŒ„ç•¶å‰è¶…åƒæ•¸
        hyperparams = cnn_agent.get_hyperparameters()
        
        # æ¯éš”ä¸€å®š episodes é€²è¡Œè©•ä¼°
        if (episode + 1) % EVAL_INTERVAL == 0:
            win_rate, avg_reward = evaluate_agent(cnn_agent, NUM_EVAL_GAMES)
            
            # è¨˜éŒ„è¨“ç·´æ•¸æ“š
            training_log['episodes'].append(episode + 1)
            training_log['win_rates'].append(win_rate)
            training_log['avg_rewards'].append(avg_reward)
            training_log['epsilon_values'].append(hyperparams['epsilon'])
            training_log['lr_values'].append(hyperparams['learning_rate'])
            
            print(f"\nEpisode {episode + 1}/{NUM_EPISODES}")
            print(f"å‹ç‡: {win_rate:.2%}")
            print(f"å¹³å‡çå‹µ: {avg_reward:.3f}")
            print(f"Epsilon: {hyperparams['epsilon']:.4f}")
            print(f"å­¸ç¿’ç‡: {hyperparams['learning_rate']:.2e}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if win_rate > best_win_rate:
                best_win_rate = win_rate
                torch.save(cnn_agent.model.state_dict(), "cnn_agent_best_annealing.pt")
                print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ï¼å‹ç‡: {win_rate:.2%}")
        
        # å®šæœŸä¿å­˜æª¢æŸ¥é»
        if (episode + 1) % SAVE_INTERVAL == 0:
            torch.save(cnn_agent.model.state_dict(), f"cnn_agent_episode_{episode + 1}_annealing.pt")
    
    # ä¿å­˜æœ€çµ‚æ¨¡å‹å’Œè¨“ç·´è¨˜éŒ„
    torch.save(cnn_agent.model.state_dict(), "cnn_agent_final_annealing.pt")
    
    # ä¿å­˜è¨“ç·´è¨˜éŒ„
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    with open(f"training_log_{timestamp}.json", 'w') as f:
        json.dump(training_log, f, indent=2)
    
    # ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_training_curves(training_log, timestamp)
    
    print(f"\nè¨“ç·´å®Œæˆï¼æœ€ä½³å‹ç‡: {best_win_rate:.2%}")
    print(f"æœ€çµ‚ epsilon: {cnn_agent.epsilon:.4f}")
    print(f"æœ€çµ‚å­¸ç¿’ç‡: {cnn_agent.lr:.2e}")


def evaluate_agent(cnn_agent, num_games=100):
    """è©•ä¼° agent çš„è¡¨ç¾"""
    # æš«æ™‚è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼ï¼ˆä¸æ¢ç´¢ï¼Œä¸è¨“ç·´ï¼‰
    original_train = cnn_agent.train
    original_epsilon = cnn_agent.epsilon
    
    cnn_agent.model.eval()
    cnn_agent.train = False
    cnn_agent.epsilon = 0.0  # ç´” exploitation
    
    wins = 0
    total_reward = 0
    
    for _ in range(num_games):
        game = big2Game()
        agents = [cnn_agent, RandomAgent(), RandomAgent(), RandomAgent()]
        
        # é‡ç½® agents
        for agent in agents:
            agent.reset()
        
        # é€²è¡ŒéŠæˆ²
        while not game.isGameOver():
            player_go, first_player, history, hand, avail_actions = game.getCurrentState()
            action = agents[player_go].step(first_player, history, hand, avail_actions)
            game.step(action)
        
        # çµ±è¨ˆçµæœ
        game.assignRewards()
        if game.rewards[0] > 0:  # CNN agent ç²å‹
            wins += 1
        total_reward += game.rewards[0]
    
    # æ¢å¾©åŸå§‹è¨­ç½®
    cnn_agent.model.train()
    cnn_agent.train = original_train
    cnn_agent.epsilon = original_epsilon
    
    win_rate = wins / num_games
    avg_reward = total_reward / num_games
    
    return win_rate, avg_reward


def plot_training_curves(training_log, timestamp):
    """ç¹ªè£½è¨“ç·´æ›²ç·š"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    episodes = training_log['episodes']
    
    # å‹ç‡æ›²ç·š
    ax1.plot(episodes, training_log['win_rates'], 'b-', linewidth=2)
    ax1.set_xlabel('Episodes')
    ax1.set_ylabel('Win Rate')
    ax1.set_title('CNN Agent Win Rate Over Time')
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)
    
    # å¹³å‡çå‹µæ›²ç·š
    ax2.plot(episodes, training_log['avg_rewards'], 'g-', linewidth=2)
    ax2.set_xlabel('Episodes')
    ax2.set_ylabel('Average Reward')
    ax2.set_title('Average Reward Over Time')
    ax2.grid(True, alpha=0.3)
    
    # Epsilon è¡°æ¸›æ›²ç·š
    ax3.plot(episodes, training_log['epsilon_values'], 'r-', linewidth=2)
    ax3.set_xlabel('Episodes')
    ax3.set_ylabel('Epsilon')
    ax3.set_title('Epsilon Decay Over Time')
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, max(training_log['epsilon_values']) * 1.1)
    
    # å­¸ç¿’ç‡è¡°æ¸›æ›²ç·š
    ax4.semilogy(episodes, training_log['lr_values'], 'm-', linewidth=2)
    ax4.set_xlabel('Episodes')
    ax4.set_ylabel('Learning Rate (log scale)')
    ax4.set_title('Learning Rate Decay Over Time')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'training_curves_{timestamp}.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"è¨“ç·´æ›²ç·šå·²ä¿å­˜ç‚º training_curves_{timestamp}.png")


if __name__ == "__main__":
    # æª¢æŸ¥æ˜¯å¦æœ‰ GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        train_with_annealing()
    except KeyboardInterrupt:
        print("\nè¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\nè¨“ç·´éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
